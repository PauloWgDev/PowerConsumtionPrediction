{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0822ab",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb30cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3c06f",
   "metadata": {},
   "source": [
    "### Por Location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beead962",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No station data was successfully parsed. Check file formats.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# If nothing parsed, exit early\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_stations:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo station data was successfully parsed. Check file formats.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 4) Concatenate all stations → one big DataFrame of daily temps\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    104\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_stations, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No station data was successfully parsed. Check file formats."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1) Helper: convert DMS (degrees/minutes/seconds + N/S/E/W) → decimal degrees\n",
    "# --------------------------------------------------------------\n",
    "def dms_to_decimal(degrees, minutes, seconds, direction):\n",
    "    \"\"\"\n",
    "    Convert DMS (degrees, minutes, seconds) + direction ('N','S','E','W') to decimal degrees.\n",
    "    \"\"\"\n",
    "    dec = float(degrees) + float(minutes)/60 + float(seconds)/3600\n",
    "    if direction in ['S', 'W']:\n",
    "        dec = -dec\n",
    "    return dec\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2) Function to parse one station CSV:\n",
    "#    - Read header lines to extract station name and DMS coordinates.\n",
    "#    - Read the day×month table, melt into (Date, Temp), attach metadata.\n",
    "# --------------------------------------------------------------\n",
    "def parse_station_csv(file_path):\n",
    "    # Read the first 6 lines (metadata + header row)\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        header_lines = [f.readline().strip() for _ in range(6)]\n",
    "    \n",
    "    # --- 2.1) Extract Station Name from line 2.  e.g. \"測站：南投  Nantou C0I460\"\n",
    "    station_line = header_lines[1]\n",
    "    station_match = re.search(r\"測站：([^ ]+)\", station_line)\n",
    "    station_name = station_match.group(1) if station_match else file_path.split('/')[-1]\n",
    "    \n",
    "    # --- 2.2) Extract DMS Latitude/Longitude from line 5.  \n",
    "    #       e.g. \"緯度：23°54'47.55〞N經度：120°40'45.06〞E\"\n",
    "    lat_lon_line = header_lines[4]\n",
    "    dms_pattern = re.compile(\n",
    "        r\"緯度：(\\d+)°(\\d+)'([\\d.]+)〞([NS])經度：(\\d+)°(\\d+)'([\\d.]+)〞([EW])\"\n",
    "    )\n",
    "    match = dms_pattern.search(lat_lon_line)\n",
    "    if match:\n",
    "        lat_deg, lat_min, lat_sec, lat_dir, lon_deg, lon_min, lon_sec, lon_dir = match.groups()\n",
    "        lat = dms_to_decimal(lat_deg, lat_min, lat_sec, lat_dir)\n",
    "        lon = dms_to_decimal(lon_deg, lon_min, lon_sec, lon_dir)\n",
    "    else:\n",
    "        lat, lon = None, None\n",
    "\n",
    "    # --- 2.3) Read the actual table. \n",
    "    # The header row is on line 6 (0‐based index row 5), so we skip the first 5 lines.\n",
    "    # That header looks like: \"Day/Month,1,2,…,12,Day/Month\"\n",
    "    df = pd.read_csv(file_path, skiprows=5)\n",
    "    \n",
    "    # Drop the last \"Day/Month\" column if pandas has named it \"Day/Month.1\"\n",
    "    if 'Day/Month.1' in df.columns:\n",
    "        df = df.drop(columns=['Day/Month.1'])\n",
    "    \n",
    "    # Rename the first column from \"Day/Month\" → \"Day\"\n",
    "    df = df.rename(columns={'Day/Month': 'Day'})\n",
    "    \n",
    "    # Melt from wide (day×month) → long (Day, Month, Temp)\n",
    "    df_long = df.melt(id_vars='Day', var_name='Month', value_name='Temp')\n",
    "    \n",
    "    # Convert Day & Month to numeric; drop any rows where Temp is missing or invalid\n",
    "    df_long['Day']   = pd.to_numeric(df_long['Day'], errors='coerce')\n",
    "    df_long['Month'] = pd.to_numeric(df_long['Month'], errors='coerce')\n",
    "    df_long = df_long.dropna(subset=['Day', 'Month', 'Temp'])\n",
    "    \n",
    "    # Build a proper datetime (year=2024) from Day/Month\n",
    "    df_long['Date'] = pd.to_datetime(\n",
    "        dict(year=2024, month=df_long['Month'].astype(int), day=df_long['Day'].astype(int)),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df_long = df_long.dropna(subset=['Date'])\n",
    "    \n",
    "    # Attach station metadata\n",
    "    df_long['Station']   = station_name\n",
    "    df_long['Latitude']  = lat\n",
    "    df_long['Longitude'] = lon\n",
    "    \n",
    "    # Keep only the columns we care about\n",
    "    return df_long[['Station', 'Latitude', 'Longitude', 'Date', 'Temp']]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3) Loop over all CSV files in /mnt/data (adjust pattern if needed)\n",
    "# --------------------------------------------------------------\n",
    "file_paths = glob.glob('C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data/*.csv')\n",
    "all_stations = []\n",
    "\n",
    "for fp in file_paths:\n",
    "    try:\n",
    "        station_df = parse_station_csv(fp)\n",
    "        all_stations.append(station_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {fp}: {e}\")\n",
    "\n",
    "# If nothing parsed, exit early\n",
    "if not all_stations:\n",
    "    raise RuntimeError(\"No station data was successfully parsed. Check file formats.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4) Concatenate all stations → one big DataFrame of daily temps\n",
    "# --------------------------------------------------------------\n",
    "df_all = pd.concat(all_stations, ignore_index=True)\n",
    "\n",
    "# Sanity check: \n",
    "# print(df_all[['Station','Latitude','Longitude','Date','Temp']].head())\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5) Compute each station’s monthly mean temperature\n",
    "# --------------------------------------------------------------\n",
    "df_all['Month'] = df_all['Date'].dt.month\n",
    "monthly_means = (\n",
    "    df_all\n",
    "    .groupby(['Station', 'Latitude', 'Longitude', 'Month'])['Temp']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot so that each station is one row, with 12 columns (Month=1…12)\n",
    "df_features = (\n",
    "    monthly_means\n",
    "    .pivot_table(\n",
    "        index=['Station', 'Latitude', 'Longitude'],\n",
    "        columns='Month',\n",
    "        values='Temp'\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns from 1→'Month_1', 2→'Month_2', … up to 12\n",
    "new_cols = ['Station', 'Latitude', 'Longitude'] + [f'Month_{m}' for m in range(1, 13)]\n",
    "df_features.columns = new_cols\n",
    "\n",
    "# If any monthly values are still NaN (e.g. Feb 29 missing), fill via forward/backfill\n",
    "df_features = df_features.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6) Run K-Means clustering on the 12 monthly-mean columns\n",
    "#    (add Latitude/Longitude if you want spatial info in the clustering)\n",
    "# --------------------------------------------------------------\n",
    "# Example: cluster purely on the 12 temperature columns\n",
    "monthly_cols = [f'Month_{m}' for m in range(1, 13)]\n",
    "k = 4  # ← you can choose any k (4 is just an example)\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df_features['ClimateCluster'] = kmeans.fit_predict(df_features[monthly_cols])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7) Show the final table: one row per station, with coords, \n",
    "#    12 monthly means, and the assigned “ClimateCluster” label.\n",
    "# --------------------------------------------------------------\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(\n",
    "    name=\"Station Climate Clusters\",\n",
    "    dataframe=df_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2a188",
   "metadata": {},
   "source": [
    "# Cluster Por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c797e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 五股 _ csv_1.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 南投 _ csv_7.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 員林 _ csv_16.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 嘉義 _ csv_9.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 基隆 _ csv_3.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 宜蘭 _ csv_12.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 恆春 _ csv_8.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 斗南 _ csv_15.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 新竹市東區 _ csv.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 板橋 _ csv.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 桃園 _ csv_4.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 深坑 _ csv.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 臺中 _ csv_6.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 臺北 _ csv_2.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 臺南 _ csv_13.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 苗栗 _ csv_5.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 蘇澳 _ csv_10.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 高雄 _ csv_11.csv', 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data\\\\逐日平均氣溫年報表_2024 麟洛 _ csv_14.csv']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Pivot so each station is a column, indexed by Date\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m df_pivot \u001b[38;5;241m=\u001b[39m df_all\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Drop any dates with missing station readings\u001b[39;00m\n\u001b[0;32m     72\u001b[0m df_pivot \u001b[38;5;241m=\u001b[39m df_pivot\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\frame.py:9339\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[1;34m(self, columns, index, values)\u001b[0m\n\u001b[0;32m   9332\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9333\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\n\u001b[0;32m   9335\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m   9336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9337\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[1;32m-> 9339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, values\u001b[38;5;241m=\u001b[39mvalues)\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:570\u001b[0m, in \u001b[0;36mpivot\u001b[1;34m(data, columns, index, values)\u001b[0m\n\u001b[0;32m    566\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m result \u001b[38;5;241m=\u001b[39m indexed\u001b[38;5;241m.\u001b[39munstack(columns_listlike)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    571\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    572\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m    573\u001b[0m ]\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\series.py:4615\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[1;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[0;32m   4570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[0;32m   4572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4611\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[0;32m   4612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4613\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 4615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstack(\u001b[38;5;28mself\u001b[39m, level, fill_value, sort)\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:517\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[1;32m--> 517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m _Unstacker(\n\u001b[0;32m    518\u001b[0m     obj\u001b[38;5;241m.\u001b[39mindex, level\u001b[38;5;241m=\u001b[39mlevel, constructor\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_expanddim, sort\u001b[38;5;241m=\u001b[39msort\n\u001b[0;32m    519\u001b[0m )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[0;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:154\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[1;34m(self, index, level, constructor, sort)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[0;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    150\u001b[0m         PerformanceWarning,\n\u001b[0;32m    151\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    152\u001b[0m     )\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_selectors()\n",
      "File \u001b[1;32mc:\\Users\\Tino\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:210\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[1;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    import glob\n",
    "    import os \n",
    "\n",
    "    # Directory where all 19 station CSV files are stored\n",
    "    # Option B (forward slashes):\n",
    "    data_directory = 'C:/Users/Tino/Documents/PowerComsuption_MachineLearning/PowerConsumtionPrediction/Raw Data/Meteorological Data'\n",
    "    pattern        = data_directory + \"/*.csv\"\n",
    "    all_files      = glob.glob(pattern)\n",
    "\n",
    "    print(\"Files found:\", all_files) \n",
    "\n",
    "    def process_station_csv(file_path):\n",
    "        \"\"\"\n",
    "        Reads a single station CSV (with a \"Day/Month\" or similar first column),\n",
    "        skips metadata rows, drops the '平均' row, and returns a DataFrame\n",
    "        of (Date, Station, Temperature).\n",
    "        \"\"\"\n",
    "        df_raw = pd.read_csv(file_path, skiprows=5, encoding='utf-8-sig')\n",
    "\n",
    "        # Identify the first column name (which indicates \"Day/Month\")\n",
    "        day_col = df_raw.columns[0]\n",
    "        \n",
    "        # If there's a duplicate 'Day/Month' column, drop it\n",
    "        duplicate_col = day_col + '.1'\n",
    "        if duplicate_col in df_raw.columns:\n",
    "            df_raw = df_raw.drop(columns=[duplicate_col])\n",
    "        \n",
    "        # Drop the row where the day column == '平均'\n",
    "        df_raw = df_raw[df_raw[day_col] != '平均']\n",
    "        \n",
    "        # Melt to long form: Day × Month → Temperature\n",
    "        df_long = df_raw.melt(id_vars=[day_col], var_name='Month', value_name='Temperature')\n",
    "        df_long = df_long.rename(columns={day_col: 'Day'})\n",
    "        \n",
    "        # Infer station name from the filename\n",
    "        station_name = file_path.split('/')[-1].replace('.csv', '').split('_')[-1]\n",
    "        df_long['Station'] = station_name\n",
    "        \n",
    "        # Convert Day and Month to numeric\n",
    "        df_long['Day'] = pd.to_numeric(df_long['Day'], errors='coerce')\n",
    "        df_long['Month'] = pd.to_numeric(df_long['Month'], errors='coerce')\n",
    "        \n",
    "        # Build a Date column for 2024; invalid dates become NaT\n",
    "        df_long['Date'] = pd.to_datetime(\n",
    "            dict(year=2024, month=df_long['Month'], day=df_long['Day']),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Drop rows where Date or Temperature is NaN\n",
    "        df_long = df_long.dropna(subset=['Date', 'Temperature'])\n",
    "        \n",
    "        # Keep only the columns we need\n",
    "        return df_long[['Date', 'Station', 'Temperature']]\n",
    "\n",
    "    # Process each CSV and concatenate\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        df_station = process_station_csv(file)\n",
    "        dfs.append(df_station)\n",
    "\n",
    "\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Pivot so each station is a column, indexed by Date\n",
    "    df_pivot = df_all.pivot(index='Date', columns='Station', values='Temperature')\n",
    "\n",
    "    # Drop any dates with missing station readings\n",
    "    df_pivot = df_pivot.dropna()\n",
    "\n",
    "    # Prepare feature matrix: rows = dates, columns = station temperatures\n",
    "    X = df_pivot.values\n",
    "\n",
    "    # Apply K-Means clustering (e.g., into 4 weather-type clusters)\n",
    "    n_clusters = 4\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_pivot['WeatherType'] = kmeans.fit_predict(X)\n",
    "\n",
    "    # Build a result DataFrame\n",
    "    result = df_pivot.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "    # Display the first several rows along with cluster assignments\n",
    "    import ace_tools as tools\n",
    "    tools.display_dataframe_to_user(\n",
    "        name=\"Combined Stations Temperature Clustering (2024)\",\n",
    "        dataframe=result.head(20)\n",
    "    )\n",
    "\n",
    "    # Also show how many days fall into each cluster\n",
    "    cluster_counts = result['WeatherType'].value_counts().sort_index()\n",
    "    print(\"Number of days in each cluster:\")\n",
    "    print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb2b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
